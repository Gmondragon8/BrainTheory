{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q learning: Atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential      \n",
    "from keras.layers import Dense, Flatten  \n",
    "from collections import deque            \n",
    "import matplotlib.pyplot as plt\n",
    "import random     \n",
    "import numpy as np\n",
    "import gym      \n",
    "env = gym.make('Breakout-v0') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo: Deep neural net, no usa pooling ya que no queremos reconocer ciertas carácterísticas aisladas sino que es importante su posición en la imagen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(40, input_shape=(2,) + env.observation_space.shape, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Flatten())       #Aplana las entradas multidimensionales\n",
    "model.add(Dense(20, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(10, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(env.action_space.n, kernel_initializer='uniform', activation='softplus')) \n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros del algorítmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observetime = 500                          \n",
    "epsilon = 0.7                              \n",
    "gamma = 0.9                                \n",
    "mb_size = 30                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones útiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotHist(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de la red, aprende de las observaciones realizadas en el juego terminado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainDQN(minibatch,state):\n",
    "    inputs_shape = (mb_size,) + state.shape[1:]\n",
    "    inputs = np.zeros(inputs_shape)\n",
    "    targets = np.zeros((mb_size, env.action_space.n))\n",
    "\n",
    "    for i in range(0, mb_size):\n",
    "        state = minibatch[i][0]\n",
    "        action = minibatch[i][1]\n",
    "        reward = minibatch[i][2]\n",
    "        state_new = minibatch[i][3]\n",
    "        done = minibatch[i][4]\n",
    "\n",
    "        inputs[i:i+1] = np.expand_dims(state, axis=0)\n",
    "        targets[i] = model.predict(state)\n",
    "        Q_sa = model.predict(state_new)\n",
    "\n",
    "        if done:\n",
    "            targets[i, action] = reward\n",
    "        else:\n",
    "            targets[i, action] = reward + gamma * np.max(Q_sa)\n",
    "\n",
    "        history = model.train_on_batch(inputs, targets)\n",
    "        #plotHist(history)\n",
    "        #if ((i%int(mb_size/3))==0): print(\"Learning...\",i)\n",
    "    #print('Fin aprendizaje')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones y aprendizaje. \n",
    "Se guardan observaciones, los estados y sus recompensas durante varios juegos.\n",
    "Con las observaciones almacenadas se entrena a la red al terminar cada juego. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Continúa aprendizaje\n",
      "Fin observaciones-aprendizaje\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10000):         #Cantidad de juegos con reseteo de memoria.\n",
    "    #print(\"Nuevo comienzo de aprendizaje\")\n",
    "    D = deque()                                \n",
    "    observation = env.reset()                    \n",
    "    obs = np.expand_dims(observation, axis=0)    \n",
    "    state = np.stack((obs, obs), axis=1)\n",
    "    done = False\n",
    "    for t in range(observetime):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, env.action_space.n, size=1)[0]\n",
    "        else:\n",
    "            Q = model.predict(state)          \n",
    "            action = np.argmax(Q)             \n",
    "        observation_new, reward, done, info = env.step(action)  \n",
    "        obs_new = np.expand_dims(observation_new, axis=0)       \n",
    "        state_new = np.append(np.expand_dims(obs_new, axis=0), state[:, :1, :], axis=1)  \n",
    "        D.append((state, action, reward, state_new, done))     \n",
    "        state = state_new   \n",
    "        if done:\n",
    "            minibatch = random.sample(D, mb_size)           \n",
    "            trainDQN(minibatch,state)\n",
    "            env.reset()           \n",
    "            obs = np.expand_dims(observation, axis=0) \n",
    "            state = np.stack((obs, obs), axis=1)\n",
    "    if ((i%100)==0): print(\"Continúa aprendizaje\")\n",
    "\n",
    "print('Fin observaciones-aprendizaje')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Jugando con la DQN entrenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del juego, recompensa total: 41\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "obs = np.expand_dims(observation, axis=0)\n",
    "state = np.stack((obs, obs), axis=1)\n",
    "done = False\n",
    "tot_reward = 0.0\n",
    "count = 0\n",
    "while not done:\n",
    "    #env.render()                    \n",
    "    \n",
    "    if np.random.rand() <= 0.1:\n",
    "        action = np.random.randint(0, env.action_space.n, size=1)[0]\n",
    "    else:\n",
    "        Q = model.predict(state)     \n",
    "        action = np.argmax(Q)        \n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    obs = np.expand_dims(observation, axis=0)\n",
    "    state = np.append(np.expand_dims(obs, axis=0), state[:, :1, :], axis=1)    \n",
    "    tot_reward = tot_reward + reward\n",
    "    count = count + 1\n",
    "    #if ((count%100)==0): print(\"Playing... Reward:%0.2f\"%(tot_reward))\n",
    "        \n",
    "    if (count==500 and tot_reward==0): \n",
    "        done = True\n",
    "        print(\"Nunca inició el juego\")\n",
    "        \n",
    "print('Fin del juego, recompensa total: {}'.format(tot_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bibliografía\n",
    "\n",
    "[Becominghuman.ai - $\"Lets bild an Atari\"$](https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26)\n",
    "\n",
    "[Neuro.cs.ut.ee](http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-12)\n",
    "\n",
    "[OpenAi Documentation](https://gym.openai.com/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
